●機械学習のしくみについてメモ書き

参考図書：Python3年生 機械学習のしくみ 体験してわかる！会話でまなべる！

2023/10/06
第1章
・人工知能は、データを「入力」すると、判断や予測を「出力」するもの。

・機械学習は、データを多く渡すと、コンピュータが自分で学習していくもの。ものが持っている特徴の法則性を学習させること。この学習により「予測」が可能となる。
　予測の精度を高めるためには、「データの量や質の良さ」が重要となってくる。つまり、機械学習では、「プログラムを書くこと」だけが重要ではなく、「良いデータを、どうすれば用意できるかを考えること」も重要となる。

・機械学習の種類
　1.教師あり学習：数値や分類を予測する学習　例）画像認識、文字認識など
  2.教師なし学習：データをまとめる学習　例）クラスタリング、次元削除など
  3.強化学習：経験してうまくなる学習

・データ分析と機械学習の違いについて
　1.データ分析：過去を説明するのが目的
　2.機械学習：未来を予測するのが目的

・機械学習の用途の違い
　1.回帰：ある値に関係する値がどんな数値になるかを予測するとき
  2.分類：あるデータが、どの分類にあてはまるかを予測するとき
　3.クラスタリング：たくさんのデータをグループ分けするとき

・機械学習のアルゴリズムでやっていることは「線を引くこと」

・うまく分けるためには、意味のある特徴量が重要
　機械学習を行うには、まず現実世界にあるものの性質や状況をデータ化して、取り込む必要があります。この「現実世界の性質や状況の測定できるデータ」のことを「特徴量」といいます。
　予測にとって意味のある特徴量、つまり「予測の根拠になるもの」を「説明変数」といいます。そして「予測される結果」のことを「目的変数」といいます。
　機械学習では、この説明変数と目的変数を使って、学習したり予測を行います。

第2章
・scikit-learnのサンプルデータセットの種類
　1.ボストンの住宅価格：load_boston()
  2.アヤメの品種：load_iris()
  3.手書き数字データ：load_digits()
  4.ワインの種類：load_wine()
  5.運動能力データセット：load_linnerud()
  6.糖尿病の進行状況：load_diabetes()
  7.乳がんの陰性/陽性：load_breast_cancer()

・アヤメのデータセット
　1.data：学習用データ
　2.feature_names：特徴量の名前
  3.target：目的の値（分類の値）
　4.target_names：目的の名前（分類の名前）
　5.DESCR：このデータセットの説明（）

・3つの品種
　1.setosa：セトサ（ヒオウギアヤメ）、北海道やアラスカに分布
  2.versicolor：バージカラー、アメリカ東部やカナダ東部に分布
  3.virginica：バージ二カ、アメリカ南東部に分布

・サンプルデータの自動生成
　1.分類用データセット（塊）：make_blobs(パラメータ)
　2.分類用データセット（三日月）：make_moons(パラメータ)
　3.分類用データセット（二重円）：make_circles(パラメータ)
  4.分類用データセット（同心円）：make_gaussian_quantiles(パラメータ)
　5.回帰用データセット：make_regression(パラメータ)
  　※特徴量がXに、その分類（目的変数）がyに返ってくる

・分類用データセット（塊）のパラメータ
  1.random_state：ランダム生成の種にする番号
  2.n_samples：データの個数
  3.n_features：特徴量の数
  4.centers：塊の数
  5.cluster_std：ばらつきの大きさ（標準偏差）

・分類用データセット（三日月）のパラメータ
  1.random_state：ランダム生成の種にする番号
  2.n_samples：データの個数
  3.noise：ノイズ

・分類用データセット（二重円）のパラメータ
  1.random_state：ランダム生成の種にする番号
  2.n_samples：データの個数
  3.noise：ノイズ

・分類用データセット（同心円）のパラメータ
  1.random_state：ランダム生成の種にする番号
  2.n_samples：データの個数
  3.n_features：特徴量の数
  4.n_classes：グループの数

・回帰用データセットのパラメータ
  1.random_state：ランダム生成の種にする番号
  2.n_samples：データの個数
  3.n_features：特徴量の数
  4.noise：ノイズ
  5.bias：y切片

2023/10/07
第3章
・機械学習では、以下のような手順で行う。
　1.データを用意する
  2.データを学習用とテスト用に分ける
  3.モデルを選んで、学習する
  4.モデルをテストする
  5.新しい値を渡して、予測する

・データを学習用をテスト用に分割する
　「特徴量（説明変数）X」と「その分類（目的変数）y」のデータを、学習用とテスト用に分類します。
　データを分割する命令はtrain_test_splitです。

　データを分割する書式
　X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)

　・X_train：学習用データの問題（説明変数）
　・y_train：学習用データの答え（目的変数）
　・X_test：テスト用データの問題（説明変数）
　・y_test：テスト用データの答え（目的変数）

  命令を実行すると、Xとyがペアでランダムにシャッフルされて、75%が学習用として、25%がテスト用として分割されて返ってきます。

・モデルを選んで、学習する
　アルゴリズムは、SVM（サポートベクターマシーン）を利用する。

　SVMで学習モデルを作り、学習用データを渡して学習する
　from sklearn import svm
  model = svm.SVC()
  model.fit(X_train, y_train)

・モデルをテストする
　まず、predict命令に「テスト用データの問題（X_test）を渡して予測」させます。
  次に、accuracy_score命令で正解率を調べる

・新しい値を渡して、予測する
　この機会学習では、2つの特徴量（説明変数）を使って学習させました。ですので予測も、2つの特徴量（説明変数）を渡して予測します。
　例として、説明変数が「1と3」のデータと説明変数が「1と2」のデータを渡して予測結果を見てみる。

・分類の状態を可視化しよう
　どのように分類できているかを見る方法としては、「グラフ上のすべての点の分類を総当たりで調べる」という方法がある。
　np.meshgrid命令を使えば、グラフをマス目状に区切った点のデータを作ることができる。この点それぞれがどの分類になるか調べて、
　plt.pcolormesh命令を使えば、グラフ全体をマス目状に色で塗りつぶすことができる。

・散布図に分類の状態を描画する関数
　plot_boundary(model, X, Y, target, xlabel, ylabel)

  ・model = 分類を行う学習済モデル（Noneにすると、散布図だけの描画になる）
　・X = X軸に使う特徴量
　・Y = Y軸に使う特徴量
  ・target = 分類の値
　・xlabel = X軸用ラベル
　・ylabel = Y軸用ラベル

第4章
・機械学習のアルゴリズム：どのような考え方で、線を引く（予測や分類を行う）のか

・回帰：どんな数値になるのかを予測したい
・分類：これは何なのかを予測したい

・回帰：線形回帰（線を引いて予測する）
　線形回帰は、予測したい状況（説明変数X）を数値で入力すると、予測結果（目的変数y）を数値で出力するアルゴリズムです。
　「予測したい状況（説明変数X）」と「予測結果（目的変数y）」に強い相関関係がある場合に使える方法です。ex.)気温とアイスクリームの売れ行き、家賃と部屋の広さ

  線形回帰では、「直線をどの角度でどの位置に引くのか」をアルゴリズムで求めるのですが、主に最小二乗法という方法が使われます。
　引いた線と実データとの誤差が一番小さくなるような線を求める方法です。

・モデルの使い方
　線形回帰のモデルは、LinearRegressionで作ります。モデルのfit命令に「説明変数X」と「目的変数y」を渡して、学習させます。

　書式：
　モデル = LinearRegression()
  モデル.fit(説明変数X, 目的変数y)

  予測結果 = モデル.predict(説明変数X)

・分類：ロジスティック回帰
　ロジスティックシグモイド関数
　「このデータの法則は、本来その線の形に戻っていくだろう」という意味では回帰だが、これによって得られる予測は、「YES(1)かNO(2)の、どちらかになるか」という分類。

　2つに分類するものを二項ロジスティック回帰分析ともいいますが、3つ以上に分類できるようにしたものを多項ロジスティック回帰分析などともいいます。

・モデルの使い方
　ロジスティック回帰のモデルは、LogisticRegressionで作ります。モデルのfit命令に「説明変数X」と「目的変数y」を渡して、学習させます。

　書式：
　モデル = LogisticRegression()
  モデル.fit(説明変数X, 目的変数y)

  予測結果 = モデル.predict(説明変数X)

2023/10/08
・分類：SVM（サポートベクターマシーン）
　「区別しやすい特徴に注目すること」⇒　SVMだけではなく、他のアルゴリズムでも重要。
　SVMのアルゴリズムでは、「境界線に近い点」に注目して考える。これを「サポートベクトル」と呼んでいる。

　線を引くとき、境界線に近い点までの距離が、なるべく遠くなるように線を引く。（自分側からも相手側からもなるべく遠い境界線を考える）
　⇒　結果的に公平な分類の境界線となる。
　
　SVMでは、これを「サポートベクトルからのマージン（余白）」を使って考える。それぞれのサポートベクトルから境界線までの距離が一番遠くなるように
　線を引くことで公平な分類の境界線を求めている。この考え方を使って、学習データに忠実に境界線を引いたものを、「ハードマージン」という。

　現実世界では、データに誤差が生じる。この多少の誤差を許容して、ゆるい線を引くほうが自然な線が引ける。
　これを「ソフトマージン」といい、機械学習では、ソフトマージンが使われている。

　「カーネルトリック」：直線で分割できないデータに対して、「2次元でだめなら、次元を増やして、別の視点で見れば解決できるかも」という考え方。

　書式：
　1.線形分類
　モデル = svm.SVC(kernel="linear")
  モデル.fit(説明変数X, 目的変数y)

  2.非線形分類
　モデル = svm.SVC(kernel="rbf", gamma="scale")
  モデル.fit(説明変数X, 目的変数y)

  予測結果 = モデル.predict(説明変数X)

・分類：決定木
　2択の質問で分岐を行い、それを繰り返していくアルゴリズム。

　書式：
　モデル = DecisionTreeClassifier(max_depth=None, random_state=0)
  モデル.fit(説明変数X, 目的変数y)

  予測結果 = モデル.predict(説明変数X)

・分類：ランダムフォレスト
　いろいろな分岐パターンの決定木を使って予測をして、その予測結果から多数決で決めるというアルゴリズム。

　書式：
　モデル = RandomForestClassifier()
  モデル.fit(説明変数X, 目的変数y)

  予測結果 = モデル.predict(説明変数X)

・分類：k-NN（k近傍法）
　近いものは仲間と考えて、近くのk個を調べて、多数決で予測するアルゴリズム。

　書式：
　モデル = kNeighborsClassifier()
  モデル.fit(説明変数X, 目的変数y)

  予測結果 = モデル.predict(説明変数X)

・クラスタリング：k-means（k平均法）
　近いものは仲間と考えて、近いもの同士でグループ分けをするアルゴリズム。
　k近傍法は「教師あり学習」の分類アルゴリズムで、k平均法は、「教師なし学習」のクラスタリングアルゴリズムです。

　k平均法では、何グループに分けたいかを指定して、データ全体を指定したグループ数に分割します。
　そのグループ分けの方法は、基本的に2つの手順を繰り返すだけです。
　1つ目は、「各重心から近い点で、グループ分けをやり直す」で、2つ目は「各グループの平均値を求めて、それを各重心に変更する」です。

　書式：
　モデル = KMeans(n_clusters=グループ数)
  モデル.fit(説明変数X, 目的変数y)

  予測結果 = モデル.predict(説明変数X)

第5章















